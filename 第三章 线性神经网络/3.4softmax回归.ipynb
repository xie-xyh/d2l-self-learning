{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归可以用于预测多少的问题，比如预测房屋被售出的价格，或者胜场数，或者患者住院的天数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，对于分类问题也感兴趣，但更多的体现的是一个概率分布问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类问题\n",
    "独热编码：一个向量，分量和类别一样多，类别对应的分量设置为0，其他所有分量设置为1\n",
    "## 网络架构\n",
    "仿射函数：解决线性模型的分类问题，需要和输出一样多的仿射函数，每个输出对应于它自己的仿射函数。\n",
    "例如，对于以下例子：\n",
    "\n",
    "$$\\begin{gathered}\n",
    "o_{1} =x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1, \\\\\n",
    "\\text{o2} =x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2, \\\\\n",
    "\\text{03} =x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b_3. \n",
    "\\end{gathered}$$\n",
    "\n",
    "每个输出$o_1、o_2、o_3$取决于所有输入$x_1、x_2、x_3、x_4$，所以softmax回归的输出层也是全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层的参数开销\n",
    "对于任何具有d个输入和q个输出的全连接层，参数开销为$O(dq)$,将d个输入转化为q个输出的成本可以减少到$O(\\frac{dq}{n})$\n",
    "## softmax运算\n",
    "如果要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1，此外我们需要一个训练的目标函数，来激励模型精准地估计概率，例如，在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半世纪上属于预测的类别。这个属性叫校准。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax可以将未规范的预测变换为非负数且总和为1，同时让模型保持可导的性质。\n",
    "\n",
    "$$\\hat{\\mathbf{y}}=\\mathrm{softmax}(\\mathbf{o})\\quad\\text{其中}\\quad\\hat{y}_j=\\frac{\\exp(o_j)}{\\sum_k\\exp(o_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管softmax是一个非线性函数，但softmax回归的输出仍然是由输入特征的仿射变换决定，因此softmax回归是一个线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小批量样本的矢量化\n",
    "## 损失函数\n",
    "### 对数似然\n",
    "\n",
    "softmax函数给出了一个向量$\\hat{\\mathbf{y}}$,我们可以将其视为\"对给定任意输入$\\mathbf{x}$的每个类的条件概率”。例如，$\\hat{y}_1=P(y=$猫$\\mid\\mathbf{x})$。假设整个数据集$\\{\\mathbf{X},\\mathbf{Y}\\}$具有$n$个样本，其中索引$i$的样本由特征向量$\\mathbf{x}^{(i)}$和独热标签向量$\\mathbf{y}^{(i)}$组成。我们可以将估计值与实际值进行比较：\n",
    "\n",
    "\n",
    "$$P(\\mathbf{Y}\\mid\\mathbf{X})=\\prod_{i=1}^nP(\\mathbf{y}^{(i)}\\mid\\mathbf{x}^{(i)}).$$\n",
    "根据最大似然估计，我们最大化$P(\\mathbf{Y}\\mid\\mathbf{X})$,相当于最小化负对数似然：\n",
    "\n",
    "\n",
    "$$-\\log P(\\mathbf{Y}\\mid\\mathbf{X})=\\sum_{i=1}^n-\\log P(\\mathbf{y}^{(i)}\\mid\\mathbf{x}^{(i)})=\\sum_{i=1}^nl(\\mathbf{y}^{(i)},\\hat{\\mathbf{y}}^{(i)}),$$\n",
    "其中，对于任何标签$\\mathbf{y}$和模型预测$\\hat{\\mathbf{y}}$,损失函数为：\n",
    "\n",
    "$$l(\\mathbf{y},\\hat{\\mathbf{y}})=-\\sum_{j=1}^qy_j\\log\\hat{y}_j.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax及其导数\n",
    "\n",
    "\n",
    "由于softmax和相关的损失函数很常见，因此我们需要更好地理解它的计算方式。 将 (3.4.3),代\n",
    "入损失 (3.4.8)中。 利用softmax的定义，我们得到：\n",
    "\n",
    "$$\\begin{aligned}l(\\mathbf{y},\\hat{\\mathbf{y}})&=-\\sum_{j=1}^qy_j\\log\\frac{\\exp(o_j)}{\\sum_{k=1}^q\\exp(o_k)}\\\\&=\\sum_{j=1}^qy_j\\log\\sum_{k=1}^q\\exp(o_k)-\\sum_{j=1}^qy_jo_j\\\\&=\\log\\sum_{k=1}^q\\exp(o_k)-\\sum_{j=1}^qy_jo_j.\\end{aligned}$$\n",
    "\n",
    "考虑相对于任何未规范化的预测$o_j$的导数，我们得到：\n",
    "\n",
    "$$\\partial_{o_j}l(\\mathbf{y},\\hat{\\mathbf{y}})=\\frac{\\exp(o_j)}{\\sum_{k=1}^q\\exp(o_k)}-y_j=\\mathrm{softmax}(\\mathbf{o})_j-y_j.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵损失\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子：\n",
    "\n",
    "假设一个模型给出了以下输出 logits（得分）：z = [2.0, 1.0, 0.1]\n",
    "这表示模型认为样本属于三个类别的原始得分。假设真实类别是类别 1（即第一个类别），我们来计算以下内容：\n",
    "\n",
    "1. **Softmax**：将 logits 转换为概率分布。\n",
    "2. **对数似然**：计算模型给出正确标签的概率的对数。\n",
    "3. **交叉熵损失**：基于模型输出的概率和真实标签计算损失。\n",
    "\n",
    "#### 第一步：计算 Softmax\n",
    "\n",
    "Softmax 将 logits 转换为概率，公式为：\n",
    "\n",
    "$$P(y = i | z) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "我们首先计算每个 logits 的指数：\n",
    "\n",
    "$$e^{2.0} \\approx 7.389, \\quad e^{1.0} \\approx 2.718, \\quad e^{0.1} \\approx 1.105$$\n",
    "\n",
    "接着计算所有 logits 指数的和：\n",
    "\n",
    "$$7.389 + 2.718 + 1.105 \\approx 11.212$$\n",
    "\n",
    "然后，计算每个类别的概率：\n",
    "\n",
    "$$P(y = 1 | z) = \\frac{e^{2.0}}{11.212} \\approx \\frac{7.389}{11.212} \\approx 0.659$$\n",
    "\n",
    "$$P(y = 2 | z) = \\frac{e^{1.0}}{11.212} \\approx \\frac{2.718}{11.212} \\approx 0.242$$\n",
    "\n",
    "$$P(y = 3 | z) = \\frac{e^{0.1}}{11.212} \\approx \\frac{1.105}{11.212} \\approx 0.099$$\n",
    "\n",
    "因此，Softmax 转换后的概率分布为：\n",
    "\n",
    "[0.659, 0.242, 0.099]\n",
    "\n",
    "#### 第二步：计算对数似然\n",
    "\n",
    "假设真实标签是类别 1，我们希望最大化模型对正确类别的预测概率。\n",
    "\n",
    "对数似然公式为：\n",
    "\n",
    "$$\\log P(y = 1 | z) = \\log(0.659) \\approx -0.417$$\n",
    "\n",
    "对数似然值为 -0.417。\n",
    "\n",
    "#### 第三步：计算交叉熵损失\n",
    "\n",
    "交叉熵损失函数的公式为：\n",
    "\n",
    "$$\\text{Loss} = -\\sum_{i=1}^{n} y_i \\log P(y_i)$$\n",
    "\n",
    "其中 $ y_i $ 是真实标签的 one-hot 编码。对于类别 1 的样本，真实标签为 `[1, 0, 0]`。因此交叉熵损失只考虑类别 1 的概率：\n",
    "\n",
    "$$\\text{Loss} = -\\log P(y = 1 | z) = -\\log(0.659) \\approx 0.417$$\n",
    "\n",
    "#### 总结：\n",
    "\n",
    "- Softmax 概率分布：[0.659, 0.242, 0.099]\n",
    "- 对数似然：-0.417\n",
    "- 交叉熵损失：0.417\n",
    "\n",
    "通过这个例子，我们可以看到：\n",
    "- **Softmax** 将 logits 转换成了每个类别的概率。\n",
    "- **对数似然** 反映了模型对正确类别的信心，值越大越好。\n",
    "- **交叉熵损失** 是我们优化时使用的目标，值越小越好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息论基础\n",
    "### 熵\n",
    "信息论的核心思想是量化数据中的信息内容，该数值被称为分布P的熵，可以由以下方程得到：\n",
    "\n",
    "$$H[P]=\\sum_j-P(j)\\log P(j).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
